{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8220aa87",
   "metadata": {},
   "source": [
    "# Week 4: Psychoacoustics\n",
    "\n",
    "This week we will explore the field of *[psychoacoustics](https://en.wikipedia.org/wiki/Psychoacoustics)*, the study of how humans perceive and interpret sound, bridging the gap between physical sound properties and subjective auditory experiences. This will form the foundation for more detailed investigations of \"vertical\" and \"horizontal\" sound perception in the coming weeks.\n",
    "\n",
    "## Introduction to Psychoacoustics\n",
    "\n",
    "Psychoacoustics is a branch of science that examines the psychological and physiological responses associated with sound, including how we detect, differentiate, and interpret auditory stimuli. It seeks to answer questions such as: Why do some sounds seem louder than others, even if they have the same physical intensity? How do we distinguish between different musical instruments playing the same note? What makes certain sounds pleasant or unpleasant?\n",
    "\n",
    "Psychoacoustics explores the relationship between the measurable, physical properties of sound (such as frequency, amplitude, and harmonic relationships) and the way these sounds are perceived by the human ear and brain (such as pitch, loudness, and timbre). Insights from psychoacoustics are applied in audio engineering, music production, hearing aid design, and the development of audio codecs (such as MP3), which exploit perceptual limitations to compress audio data efficiently.\n",
    "\n",
    "The roots of psychoacoustics can be traced back to the 19th century, with foundational work by scientists such as [Hermann von Helmholtz](https://en.wikipedia.org/wiki/Hermann_von_Helmholtz) (1821–1894), who investigated the sensations of tone and the physical basis of music. Later, [S. S. Stevens](https://en.wikipedia.org/wiki/Stanley_Smith_Stevens)  (1906–1973) contributed to the development of psychophysical scaling, introducing methods to quantify the relationship between stimulus and perception (e.g., the Stevens' Power Law for loudness perception). Over time, psychoacoustics has evolved into a multidisciplinary field, integrating insights from physics, psychology, neuroscience, and engineering.\n",
    "\n",
    "## Anatomy of the Ear\n",
    "\n",
    "The human ear is a remarkably complex organ that enables us to detect sounds and maintain our sense of balance. It is divided into three main sections: the outer ear, middle ear, and inner ear. Each part plays a distinct and crucial role in the auditory process.\n",
    "\n",
    "### The outer ear\n",
    "\n",
    "The *[outer ear](https://en.wikipedia.org/wiki/Outer_ear)* consists of the visible part called the *pinna* and the *ear canal*. Its primary function is to collect sound waves from the environment and funnel them toward the *eardrum* (sometimes call the *tympanic membrane*). The unique shape of the pinna helps us localize the direction of sounds. When sound waves reach the eardrum, they cause it to vibrate.\n",
    "\n",
    "![Outer Ear](https://upload.wikimedia.org/wikipedia/commons/4/40/Ear-anatomy-text-small-en.svg)  \n",
    "*Image Source: [Wikipedia - Outer Ear](https://en.wikipedia.org/wiki/Outer_ear)*\n",
    "\n",
    "The eardrum  in the human ear functions much like the membrane in a microphone. Both act as sensitive barriers that vibrate in response to incoming sound waves. When sound waves enter the ear canal, they strike the eardrum, causing it to vibrate. These vibrations are then transmitted through the ossicles to the inner ear, where they are converted into electrical signals for the brain to interpret. In a microphone, sound waves hit a thin, flexible membrane (diaphragm), causing it to vibrate. These vibrations are converted into electrical signals, which can then be amplified, recorded, or transmitted. As such, both the ear drum and the microphone membrane convert air pressure variations (sound waves) into mechanical vibrations. Both also serve as the first step in transforming acoustic energy into a form that can be further processed; biologically in the ear, electronically in the microphone.\n",
    "\n",
    "### The middle ear\n",
    "\n",
    "Beyond the eardrum lies the *[middle ear](https://en.wikipedia.org/wiki/Middle_ear)*, which contains three tiny bones known as the *ossicles*: the malleus, incus, and stapes. These bones act as a mechanical lever system, amplifying the vibrations from the eardrum and transmitting them to the oval window, a membrane-covered opening to the inner ear. This amplification is essential for efficiently transferring sound energy from air to the fluid-filled inner ear.\n",
    "\n",
    "![Middle Ear](https://upload.wikimedia.org/wikipedia/commons/8/81/Blausen_0330_EarAnatomy_MiddleEar.png)  \n",
    "*Image Source: [Wikipedia - Middle Ear](https://en.wikipedia.org/wiki/Middle_ear)*\n",
    "\n",
    "This process is analogous to how a microphone connected to a preamplifier works. The vibrations from a microphone membrane are weak and are typically run through a *preamplifier* to boost the signal to a level suitable for further processing or recording. Similarly, the ossicles amplify the mechanical vibrations from the eardrum, ensuring that the signal is strong enough to be effectively transmitted into the inner ear for further processing by the auditory system.\n",
    "\n",
    "### The inner ear\n",
    "\n",
    "The *[inner ear](https://en.wikipedia.org/wiki/Inner_ear)* is where the mechanical vibrations are transformed into electrical signals that the brain can interpret as sound. The main structure responsible for this is the *[cochlea](https://en.wikipedia.org/wiki/Cochlea)*, a spiral-shaped, fluid-filled organ lined with thousands of tiny hair cells. As vibrations travel through the cochlear fluid, they cause the hair cells to move, generating nerve impulses that are sent to the brain via the auditory nerve.\n",
    "\n",
    "![Inner Ear](https://upload.wikimedia.org/wikipedia/commons/1/14/Blausen_0329_EarAnatomy_InternalEar.png)  \n",
    "*Image Source: [Wikipedia - Inner Ear](https://en.wikipedia.org/wiki/Inner_ear)*\n",
    "\n",
    "The cochlea in the inner ear functions similarly to an analog-to-digital converter (ADC) in audio technology. Just as an ADC transforms continuous analog sound waves into discrete digital signals that can be processed by computers, the cochlea converts mechanical vibrations from sound into electrical nerve impulses that the brain can interpret. Inside the cochlea, thousands of hair cells respond to specific frequencies, effectively performing a biological form of frequency analysis and encoding the intensity and timing of sounds. This process is analogous to how an ADC samples and quantizes audio signals, enabling the transmission and interpretation of complex auditory information in a form the brain can understand.\n",
    "\n",
    "The inner ear also contains the [vestibular system](https://en.wikipedia.org/wiki/Vestibular_system), which is crucial for maintaining balance and spatial orientation.\n",
    "\n",
    "\n",
    "### Human auditory perception vs machine perception\n",
    "\n",
    "Although they are different in many ways, it can be helpful to think about the human auditory system as analogous to an analog-to-digital converter (ADC). Both systems transform analog signals (sound waves) into a format that can be interpreted (heard or processed) by the brain or digital devices.\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Sound Waves] --> B[Outer Ear]\n",
    "    B --> B1[Pinna]\n",
    "    B --> B2[Ear Canal]\n",
    "    B --> C[Middle Ear]\n",
    "    C --> C1[Eardrum (Tympanic Membrane)]\n",
    "    C --> C2[Ossicles (Malleus, Incus, Stapes)]\n",
    "    C --> D[Inner Ear]\n",
    "    D --> D1[Cochlea]\n",
    "    D1 --> D2[Hair Cells]\n",
    "    D1 --> D3[Basilar Membrane]\n",
    "    D --> E[Auditory Nerve]\n",
    "    E --> F[Brain]\n",
    "    F --> F1[Auditory Cortex (Sound Processing)] \n",
    "```\n",
    "In comparison, capturing digital sound uses this signal chain: \n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Sound Waves] --> B[Microphone]\n",
    "    B --> C[Pre-Amplifier]\n",
    "    C --> D[Sampling Process]\n",
    "    D --> E[Quantization]\n",
    "    E --> F[Digital Output]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d00f72e",
   "metadata": {},
   "source": [
    "## Basic principles of sound perception\n",
    "\n",
    "### Frequency\n",
    "- **Infrasound**: Below 20 Hz. [Learn more](https://en.wikipedia.org/wiki/Infrasound)\n",
    "- **Audible Sound**: 20 Hz to 20,000 Hz. [Learn more](https://en.wikipedia.org/wiki/Hearing_range)\n",
    "- **Ultrasound**: Above 20,000 Hz. [Learn more](https://en.wikipedia.org/wiki/Ultrasound)\n",
    "\n",
    "### Instruments\n",
    "- **Piano**: 32 Hz to 4186 Hz. [Learn more](https://en.wikipedia.org/wiki/Piano)\n",
    "- **Saxophone**: Known for its rich tone. [Learn more](https://en.wikipedia.org/wiki/Saxophone)\n",
    "- **Just Noticeable Differences (JND)**: The smallest detectable frequency change. [Learn more](https://en.wikipedia.org/wiki/Just-noticeable_difference)\n",
    "\n",
    "## Loudness\n",
    "- **Equal-Loudness Contours**: Show how loudness perception varies. [Learn more](https://en.wikipedia.org/wiki/Equal-loudness_contour)\n",
    "\n",
    "The Fletcher-Munson curves, also known as equal-loudness contours, illustrate how human hearing sensitivity varies with frequency and loudness levels.\n",
    "\n",
    "![Fletcher-Munson Curves](https://upload.wikimedia.org/wikipedia/commons/4/4c/Lindos1.svg)\n",
    "*Image Source: [Wikipedia - Equal-loudness contour](https://en.wikipedia.org/wiki/Equal-loudness_contour)*\n",
    "\n",
    "- **Threshold of Hearing**: Quietest perceivable sound. [Learn more](https://en.wikipedia.org/wiki/Absolute_threshold_of_hearing)\n",
    "\n",
    "![Threshold of Hearing](https://upload.wikimedia.org/wikipedia/commons/d/da/Average_click-evoked_waveforms_and_Average_hearing_thresholds_for_younger_and_older_adults.jpg)\n",
    "*Image Source: [Wikipedia - Hearing Thresholds](https://en.wikipedia.org/wiki/Absolute_threshold_of_hearing)*\n",
    "\n",
    "#### Hysteresis\n",
    "Hysteresis refers to the phenomenon where the response of a system depends not only on its current state but also on its past states. In psychoacoustics, this can manifest in auditory perception, where the perception of a sound may be influenced by previously heard sounds or stimuli. This concept is crucial in understanding how the auditory system adapts and reacts over time to varying acoustic environments.\n",
    "\n",
    "![Hysteresis](https://upload.wikimedia.org/wikipedia/en/e/e2/Hysteresis.png)\n",
    "*Image Source: [Wikipedia - Hysteresis](https://en.wikipedia.org/wiki/Hysteresis)*\n",
    "\n",
    "- **Threshold of Pain**: Sounds above 120 dB. [Learn more](https://en.wikipedia.org/wiki/Sound_pressure)\n",
    "\n",
    "#### Time\n",
    "- **Critical Bands**: Frequency ranges where masking occurs. [Learn more](https://en.wikipedia.org/wiki/Critical_band)\n",
    "- **Temporal Masking**: One sound obscures another. [Learn more](https://en.wikipedia.org/wiki/Masking_(audio))\n",
    "\n",
    "![Audio Masking Graph](https://upload.wikimedia.org/wikipedia/commons/e/eb/Audio_Mask_Graph.png)\n",
    "*Image Source: [Wikipedia - Audio Masking](https://en.wikipedia.org/wiki/Masking_(audio))*\n",
    "\n",
    "#### Space\n",
    "- **Spatial Hearing**: Localizing sound sources. [Learn more](https://en.wikipedia.org/wiki/Sound_localization)\n",
    "\n",
    "\n",
    "### Filtering\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e395af",
   "metadata": {},
   "source": [
    "## Psychoacoustics in technology\n",
    "\n",
    "### Audio Visualizations\n",
    "\n",
    "- **Waveform**: A visual representation of amplitude over time. [Learn more](https://en.wikipedia.org/wiki/Waveform)\n",
    "- **Spectrogram**: Displays frequency content over time. [Learn more](https://en.wikipedia.org/wiki/Spectrogram)\n",
    "- **Log Mel Spectrogram**: Mimics human hearing by applying the Short-Time Fourier Transform (STFT), Mel band-pass filters, and a logarithmic transformation to represent audio on a decibel scale. Widely used in audio-related tasks for its perceptual relevance. [Learn more](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)\n",
    "- **MFCCs (Mel-Frequency Cepstral Coefficients)**: Extracts and compresses audio features by applying the Discrete Cosine Transform (DCT) to the Log Mel spectrum. Commonly used in speech processing, music classification, and music information retrieval. [Learn more](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)\n",
    "- **CQT (Constant-Q Transform)**: Uses a logarithmic frequency scale with exponentially spaced center frequencies and varying filter bandwidths. Ideal for musical note frequency extraction and analysis. [Learn more](https://en.wikipedia.org/wiki/Constant-Q_transform)\n",
    "\n",
    "### Symbolic Representations\n",
    "\n",
    "#### MIDI\n",
    "MIDI (Musical Instrument Digital Interface) is a standard protocol for communicating musical performance data between electronic instruments and computers. It encodes information such as note pitch, velocity, duration, and control changes. [Learn more](https://en.wikipedia.org/wiki/MIDI)\n",
    "\n",
    "#### ABC Notation\n",
    "ABC Notation is a text-based music notation system that uses ASCII characters to represent musical scores. It is widely used for folk and traditional music due to its simplicity and compatibility with text-based tools. [Learn more](https://en.wikipedia.org/wiki/ABC_notation)\n",
    "\n",
    "#### REMI\n",
    "REMI (REvamped MIDI-derived events) is an enhanced representation of MIDI data designed to better capture musical rhythm and structure. It introduces features like Note Duration events, Bar and Position tokens, and Tempo events, making it suitable for music generation tasks. [Learn more](https://arxiv.org/abs/2002.00212)\n",
    "\n",
    "#### MusicXML\n",
    "MusicXML is an XML-based format for representing Western music notation. It encodes detailed musical elements such as notes, rests, articulations, and dynamics, making it ideal for sharing and analyzing sheet music. [Learn more](https://en.wikipedia.org/wiki/MusicXML)\n",
    "\n",
    "#### Piano Roll\n",
    "The Piano Roll is a visual representation of music, where time is displayed on the horizontal axis and pitch on the vertical axis. Notes are represented as rectangles, with their length indicating duration. It is commonly used in digital audio workstations (DAWs) for music editing and analysis. [Learn more](https://en.wikipedia.org/wiki/Piano_roll)\n",
    "\n",
    "#### Note Graph\n",
    "A Note Graph is a graph-based representation of musical scores, where nodes represent notes and edges capture relationships such as sequence, onset, and sustain. This approach provides a structured way to analyze and model complex musical relationships. [Learn more](https://arxiv.org/abs/2006.05417)\n",
    "\n",
    "- **[Ideas Roadshow: Believing Your Ears - Auditory Illusions](https://ideasroadshow.com/items/believing-your-ears-auditory-illusions)**  \n",
    "    *Diana Deutsch in conversation with Howard Burton* (2015), Open Agenda Publishing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0062bd73",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
